{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Data Uploader for SRW Datasets to Cloud Data Storage\n",
    "\n",
    "### __Purpose:__ \n",
    "\n",
    "The purpose of this program is to transfer the Unified Forecast Sytstem Short-Range Weather Application (UFS SRW Application) fixed and input model datasets residing within the RDHPCS to cloud data storage via chaining API calls to communicate with its cloud data storage bucket. The program will support the data required for the current UFS SRW Application.\n",
    "\n",
    "According to Amazon AWS, the following conditions need to be considered when transferring data to cloud data storage:\n",
    "- Largest object that can be uploaded in a single PUT is 5 GB.\n",
    "- Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB.\n",
    "- For objects larger than 100 MB, Amazon recommends using the Multipart Upload capability.\n",
    "- The total volume of data in a cloud data storage bucket are unlimited.\n",
    "\n",
    "Tools which could be be utilized to perform data transferring & partitioning (Multipart Upload/Download) are: \n",
    "- AWS SDK\n",
    "- AWS CLI\n",
    "- AWS S3 REST API\n",
    "\n",
    "All of the AWS provided tools are built on Boto3. \n",
    "\n",
    "In this demontration, the framework will implement Python AWS SDK for transferring the UFS SRW application fixed and input model datasets from the RDHPCS, Orion, to the cloud data storage with low latency. \n",
    "\n",
    "The AWS SDK will be implemented for the following reasons:\n",
    "- To integrate with other python scripts.\n",
    "- AWS SDK carries addition capabilities/features for data manipulation & transferring compare to the aforementioned alternate tools.\n",
    "\n",
    "### __Capabilities:__ \n",
    "\n",
    "The framework will be able to perform the following actions:\n",
    "\n",
    "- Multi-threading & partitioning to the datasets to assist in the optimization in uploading performance of the datasets from on-prem to cloud. \n",
    "\n",
    "\n",
    "### __Datasets to Transfer:__\n",
    "The following conditions must be considered when storing the SRW data in cloud:\n",
    "\n",
    "- As of 05/2022, datasets to be stored in cloud need to support the SRW cases featured within the UFS SRW release version 2.0. \n",
    "- The datasets to be stored in cloud will include the fixed and input model datasets residing on the RDHPCS platform, Orion.\n",
    "\n",
    "| SRW Release Version | Fixed Data Location (on Orion) | Input Model Data Location (on Orion) |\n",
    "| :- | :- | :-: |\n",
    "| 2.0| /noaa/fv3-cam/UFS_SRW_App/develop/fix.tar | /noaa/fv3-cam/UFS_SRW_App/develop/input_model_data.tar |\n",
    "\n",
    "### __Environment Setup:__\n",
    "\n",
    "1. Install miniconda on your machine. Note: Miniconda is a smaller version of Anaconda that only includes conda along with a small set of necessary and useful packages. With Miniconda, you can install only what you need, without all the extra packages that Anaconda comes packaged with:\n",
    "\n",
    "Download latest Miniconda (e.g. 3.9 version):\n",
    "- __wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Check integrity downloaded file with SHA-256:\n",
    "- __sha256sum Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Reference SHA256 hash in following link: https://docs.conda.io/en/latest/miniconda.html\n",
    "\n",
    "Install Miniconda in Linux:\n",
    "- __bash Miniconda3-py39_4.9.2-Linux-x86_64.sh__\n",
    "\n",
    "Next, Miniconda installer will prompt where do you want to install Miniconda. Press ENTER to accept the default install location i.e. your $HOME directory. If you don't want to install in the default location, press CTRL+C to cancel the installation or mention an alternate installation directory. If you've chosen the default location, the installer will display “PREFIX=/var/home/<user>/miniconda3” and continue the installation.\n",
    "\n",
    "For installation to take into effect, run the following command: \n",
    "- __source ~/.bashrc__\n",
    "\n",
    "Next, you will see the prefix (base) in front of your terminal/shell prompt. Indicating the conda's base environment is activated.\n",
    "\n",
    "2.\tOnce you have conda installed on your machine, perform the following to create a conda environment:\n",
    "\n",
    "To create a new environment (if a YAML file is not provided)\n",
    "- __conda create -n [Name of your conda environment you wish to create]__\n",
    "\n",
    "__(OR)__\n",
    "\n",
    "To ensure you are running Python 3.9:\n",
    "- __conda create -n myenv Python=3.9__\n",
    "\n",
    "__(OR)__\n",
    "\n",
    "To create a new environment from an existing YAML file (if a YAML file is provided):\n",
    "- __conda env create -f environment.yml__\n",
    "\n",
    "__*Note:__ A .yml file is a text file that contains a list of dependencies, which channels a list for installing dependencies for the given conda environment. For the code to utilize the dependencies, you will need to be in the directory where the environment.yml file lives.\n",
    "\n",
    "4.\tActivate the new environment via: __conda activate [Name of your conda environment you wish to activate]__\n",
    "\n",
    "5.\tVerify that the new environment was installed correctly via: __conda info --env__\n",
    "\n",
    "__*Note:__\n",
    "- From this point on, must activate conda environment prior to .py script(s) or jupyter notebooks execution\n",
    "using the following command: __conda activate__\n",
    "- To deactivate a conda environment: \n",
    "    - __conda deactivate__\n",
    "\n",
    "#### ___Link Home Directory to Dataset Location on RDHPCS Platform___ \n",
    "\n",
    "6.\tUnfortunately, there is no way to navigate to the /work/ filesystem from within the Jupyter interface. The best way to workaround is to create a symbolic link in your home folder that will take you to the /work/ filesystem. Run the following command from a linux terminal on Orion to create the link: \n",
    "\n",
    "    - __ln -s /work /home/[Your user account name]/work__\n",
    "\n",
    "Now, when you navigate to the __/home/[Your user account name]/work__ directory in Jupyter, it will take you to the __/work__ folder. Allowing you to obtain any data residing within the __/work__ filesystem that you have permission to access from Jupyter. This same procedure will work for any filesystem available from the root directory. \n",
    "\n",
    "__*Note:__ On Orion, user must sym link from their home directory to the main directory containing the datasets of interest.\n",
    "\n",
    "#### ___Open & Run Data Analytics Tool on Jupyter Notebook___\n",
    "\n",
    "7.\tOpen OnDemand has a built-in file explorer and file transfer application available directly from its dashboard via ...\n",
    "    - Login to https://orion-ood.hpc.msstate.edu/ \n",
    "    - In the Open OnDemand Interface, select __Interactive Apps__ > __Jupyter Notbook__\n",
    "    - Set the following configurations to run Jupyter:\n",
    "\n",
    "\n",
    "#### ___Additonal Information___\n",
    "\n",
    "__To create a .yml file, execute the following commands:__\n",
    "\n",
    "- Activate the environment to export: \n",
    "    - __conda activate myenv__\n",
    "\n",
    "- Export your active environment to a new file:\n",
    "    - __conda env export > [ENVIRONMENT FILENAME].yml__\n",
    "\n",
    "\n",
    "### __Reference(s)__\n",
    "Latest UFS SRW Application Guide:\n",
    "- https://ufs-srweather-app.readthedocs.io/en/latest/InputOutputFiles.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Data Locality Extractor from Source\n",
    "\n",
    "Initially, cases were to be defined for the UFS SRW application release version 2.0 to determine the datasets required to support the UFS SRW application release version however it was suggested during an AUS weekly meeting to transfer all data within two tar formatted SRW dataset folders regardless as to whether a dataset is required/not required for the aforementioned UFS SRW application release version -- this is due to the unknown timing of when the SRW dataset locations will be standardized across all RDHPCS. \n",
    "\n",
    "As of 05/04/22, these tar formatted SRW dataset folders feature the standardized dataset locations according to on-prem SRW data maintainer, Michael Kavulich. As of 05/04/22, there are a few PRs (UFS SRW Application Issue #231, Issue #716, Issue #724) that need to go in first before these dataset locations changes will be implemented. \n",
    "\n",
    "As a result, all data within the two tar formatted SRW dataset folders on Orion (regardless as to whether a dataset in the tar folders are required/not required for the aforementioned UFS SRW application release version) were transferred to the cloud data storage in this demonstration to support the following developing UFS SRW release version:\n",
    "\n",
    "| SRW Release Version | Fixed Data Location (on Orion) | Input Model Data Location (on Orion) |\n",
    "| :- | :- | :-: |\n",
    "| 2.0| /noaa/fv3-cam/UFS_SRW_App/develop/fix.tar | /noaa/fv3-cam/UFS_SRW_App/develop/input_model_data.tar |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain directories for the datasets requested by the user as listed within the WE2E csv file.\n",
    "The importance of this function is to obtain only the dataset required for SRW cases requested by a given user.\n",
    "Recall, the demo is to transfer all data within two tar formatted SRW dataset folders regardless as to whether a dataset is required/not required for a user's UFS SRW application release version. By using the following function, a user can request the datasets that is applicable to their SRW release version needs -- rather than the full datasets within the SRW tar folders. This feature is a future capability as SRW development continues within this project program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "    \n",
    "#     # Module for extracting data from source.\n",
    "#     from read_srw_we2e_cases import TransferCaseData\n",
    "    \n",
    "#     # Model's Date and times requested by user.\n",
    "#     fv3gfs_ts, gsmgfs_ts, hrrr_ts, rap_ts, nam_ts = TransferCaseData(srw_cases_fn = 'WE2E Cases and Locations.xlsx').read_srw_cases()\n",
    "    \n",
    "#     # FV3LAM pregen grids requested by user. NOTE: (04.22.22) Currently contains unique fixed files per FV3LAM pregen grids folder on Orion\n",
    "#     grids_list = TransferCaseData(srw_cases_fn = 'WE2E Cases and Locations.xlsx').read_srw_grids()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep extraction of resolutions if SRW fixed files data strucutre remains as the standard data structure -- otherwise, modify appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep if the data structure for SRW fixed files remains otherwise modify appropriately.\n",
    "# import re\n",
    "# import itertools\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "\n",
    "# grid_key = []\n",
    "# res_list = []\n",
    "# grid2res = defaultdict(list)\n",
    "# for t in grids_list:\n",
    "    \n",
    "#     #grid_key.append(re.findall('([a-zA-Z_ ]*)\\d*.*', t)[0])\n",
    "    \n",
    "#     if 'RRFS_CONUS_' in t:\n",
    "#         grid2res['RRFS_CONUS'].append(re.findall(r'(\\d+)km', t)[0])\n",
    "#     elif 'SUBCONUS_' in t:\n",
    "#         grid2res['SUBCONUS'].append(re.findall(r'(\\d+)km', t)[0])\n",
    "#     elif 'RRFS_CONUScompact' in t:\n",
    "#         grid2res['RRFS_CONUScompact'].append(re.findall(r'(\\d+)km', t)[0])\n",
    "#     elif 'ESG' in t:\n",
    "#         grid2res['ESG'].append([x for x[0] in re.findall(r'(\\d+)km', t) if x!=[]])\n",
    "#     elif 'GFDL' in t:\n",
    "#         grid2res['GFDL'].append([x for x[0] in re.findall(r'(\\d+)km', t) if x!=[]])\n",
    "# grid2res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set resolutions to filter fixed files to.\n",
    "# rrfs_conus_res = grid2res['RRFS_CONUS']\n",
    "# rrfs_subconus_res = grid2res['SUBCONUS']\n",
    "# rrfs_conus_compact_res = grid2res['RRFS_CONUScompact']\n",
    "# esg_res = grid2res['ESG'][0]\n",
    "# gfdl_res = grid2res['GFDL'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read SRW tar folders and its file directory content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    \n",
    "    # Module for extracting data from source.\n",
    "    from get_srw_data import GetSrwData\n",
    "    \n",
    "    # Source SRW data\n",
    "    linked_home_dir = \"/home/schin/work\"\n",
    "    fix_data_dir = linked_home_dir + \"/noaa/fv3-cam/UFS_SRW_App/develop/fix.tar\"\n",
    "    input_model_data_dir = linked_home_dir + \"/noaa/fv3-cam/UFS_SRW_App/develop/input_model_data.tar\"\n",
    "    \n",
    "    # Instantiate SRW uploader\n",
    "    srw_uploader = GetSrwData(None, None, fix_data_dir, input_model_data_dir)\n",
    "    \n",
    "    # List all data directories from sources (filtered)\n",
    "    ma_data_list = srw_uploader.ma_data_list\n",
    "    fix_data_list = srw_uploader.fix_data_list\n",
    "        \n",
    "    # SRW input model analysis & fixed data file locations (filtered)\n",
    "    srw_ma_data_dirs = srw_uploader.ma_file_dirs\n",
    "    srw_fix_data_dirs = srw_uploader.fix_file_dirs\n",
    "    \n",
    "    # Select model analysis files based on external model it was generated by (filtered)\n",
    "    srw_ma_dict = srw_uploader.partition_ma_datasets \n",
    "    srw_fix_dict = srw_uploader.partition_fixed_datasets \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input model data files based on external model it was generated by.\n",
    "srw_ma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input fixed files.\n",
    "srw_fix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all input model data directories from MA source (filtered)\n",
    "ma_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all fix data directories from fix source (filtered)\n",
    "fix_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: Multipart Upload of Extracted Data to Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload all fixed datasets residing in SRW tar folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     from progress_bar import ProgressPercentage\n",
    "#     from upload_data import UploadData\n",
    "    \n",
    "#     # Upload input fixed data.\n",
    "#     uploader_wrapper = UploadData(srw_fix_dict, use_bucket='srw')\n",
    "#     uploader_wrapper.upload_files2cloud()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload all input model datasets residing in SRW tar folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     from progress_bar import ProgressPercentage\n",
    "#     from upload_data import UploadData\n",
    "    \n",
    "#     #Upload input model data.\n",
    "#     uploader_wrapper = UploadData(srw_ma_dict, use_bucket='srw')\n",
    "#     uploader_wrapper.upload_files2cloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidated Demo: Extract Data Localities & Upload to Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract & upload all SRW's datasets (fixed data + input model data) to cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    from progress_bar import ProgressPercentage\n",
    "    from upload_data import UploadData\n",
    "    from transfer_srw_data import TransferSrwData\n",
    "    \n",
    "    # Obtain directories & upload to cloud for all the fix and model input SRW datasets\n",
    "    srw_xfer = TransferSrwData(linked_home_dir=\"/home/schin/work\", platform=\"orion\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Upload a Single Data File of Interest\n",
    "\n",
    "**TODO:** Is there an interest to transfer the readme file: input_model_data/README_input_model_data.txt ? May Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     from progress_bar import ProgressPercentage\n",
    "#     from upload_data import UploadData\n",
    "\n",
    "#     # Upload a Single Data File of Interest\n",
    "#     uploader_wrapper = UploadData(file_relative_dirs=None, use_bucket='srw')\n",
    "#     file_dir = '###/###/[filename].[file_format]'\n",
    "#     uploader_wrapper.upload_single_file(file_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Delete a File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     from progress_bar import ProgressPercentage\n",
    "#     from upload_data import UploadData\n",
    "#     uploader_wrapper = UploadData(file_relative_dirs=None, use_bucket='srw')\n",
    "#     file_dir = '###/###/[filename].[file_format]'\n",
    "#     key_path = file_dir\n",
    "#     uploader_wrapper.purge(key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Delete Objects with Key Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     from progress_bar import ProgressPercentage\n",
    "#     from upload_data import UploadData\n",
    "#     uploader_wrapper = UploadData(file_relative_dirs=None, use_bucket='srw')\n",
    "#     key_prefix = '###'\n",
    "#     uploader_wrapper.purge_by_keyprefix(key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Copy Objects & Delete with Key Prefix\n",
    "AWS CLI copies the objects to the target folder and then removes the original file. There is no “move” action in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     from progress_bar import ProgressPercentage\n",
    "#     from upload_data import UploadData\n",
    "#     uploader_wrapper = UploadData(file_relative_dirs=None, use_bucket='srw')\n",
    "#     source_key_path = '####' \n",
    "#     new_key_path = '###'\n",
    "#     uploader_wrapper.rename_s3_keys(source_key_path, new_key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of all object keys in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     from progress_bar import ProgressPercentage\n",
    "#     from upload_data import UploadData\n",
    "#     uploader_wrapper = UploadData(file_relative_dirs=None, use_bucket='srw')\n",
    "#     all_bucket_objects = uploader_wrapper.get_all_s3_keys()\n",
    "# all_bucket_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write list to text file and save to directory\n",
    "# with open('filename].[file_format]', 'w') as f:\n",
    "#     for item in all_bucket_objects:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud_xfer",
   "language": "python",
   "name": "cloud_xfer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
